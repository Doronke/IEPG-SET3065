{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0ea2c5af",
      "metadata": {
        "id": "0ea2c5af"
      },
      "source": [
        "# Importing Useful Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11dfda10",
      "metadata": {
        "id": "11dfda10"
      },
      "source": [
        "-  Simbench installation through pip: <b> pip install simbench </b>(for more information or non pip installation https://simbench.readthedocs.io/en/stable/about/installation.html)\n",
        "-  Pandapower installation through pip: <b> pip install pandapower </b> (for more information https://www.pandapower.org/)  \n",
        "-  Sklearn installation through pip: <b> pip install -U scikit-learn </b> (for more information https://scikit-learn.org/stable/install.html)\n",
        "-  Matplotlib installation through pip: <b> python -m pip install -U matplotlib </b> (for more information https://matplotlib.org/stable/install/index.html)          \n",
        "-  Pandas installation through pip: <b> pip install pandas </b>\n",
        "-  Numpy installation through pip: <b> pip install numpy </b>                                          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b373ab9",
      "metadata": {
        "is_executing": true,
        "id": "3b373ab9"
      },
      "outputs": [],
      "source": [
        "import simbench as sb\n",
        "import pandapower.timeseries as ts\n",
        "from pandapower.timeseries.data_sources.frame_data import DFData\n",
        "from pandapower.control.controller.const_control import ConstControl\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "# from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as clrs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c4959c6",
      "metadata": {
        "id": "9c4959c6"
      },
      "source": [
        "# Dataset loading: Importing Network to be Used"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b38f5cf8",
      "metadata": {
        "id": "b38f5cf8"
      },
      "source": [
        "Simbench allows us to obtain networks with profiles for 1 year of 15 minutes timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d02262eb",
      "metadata": {
        "scrolled": true,
        "id": "d02262eb"
      },
      "outputs": [],
      "source": [
        "grid_code = '1-HV-urban--0-no_sw'\n",
        "net = sb.get_simbench_net(grid_code)     # Get network\n",
        "print(net)                               # Print Netowrk to see Components\n",
        "profiles = sb.get_absolute_values(net, profiles_instead_of_study_cases=True)  # Get full profiles with 1 year information on the network components\n",
        "\n",
        "# Please Ignore the Returned Warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cabab94",
      "metadata": {
        "id": "7cabab94"
      },
      "source": [
        "Get load active and reactive powers, generator active powers and per unit voltages.Those are the sets of features of the input data. Each column of those sets represents a different load/static generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62345d94",
      "metadata": {
        "scrolled": true,
        "id": "62345d94"
      },
      "outputs": [],
      "source": [
        "# Obtain the Profiles\n",
        "load_p = profiles[('load','p_mw')]\n",
        "load_q = profiles[('load','q_mvar')]\n",
        "\n",
        "sgen_p = profiles[('sgen','p_mw')]\n",
        "print(f\"First 5 timesteps have loads with active powers in MW of: {load_p.head()}\")\n",
        "# For Computational Time Reduction, only use data from the first 2 months (leap year)\n",
        "load_p = load_p.iloc[:4*24*60, :]\n",
        "load_q = load_q.iloc[:4*24*60, :]\n",
        "sgen_p = sgen_p.iloc[:4*24*60, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d660ede",
      "metadata": {
        "id": "6d660ede"
      },
      "source": [
        "# Task 1: Generic Information Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8d99516",
      "metadata": {
        "id": "f8d99516"
      },
      "source": [
        "1) Find the maximum active power consumption per load within the load profile (per column) <br>\n",
        "2) Find the maximum active power generation from the static generators profile <br>\n",
        "3) Find the maximum apparent power consumption magnitude from the load no.0 active and reactive power consumption profiles (Hint: Load no.0 is the 1st Column of the load_p and load_q dataframes). In your result, also include the timestep index of the found maximum apparent power.  <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f41ccf51",
      "metadata": {
        "id": "f41ccf51"
      },
      "outputs": [],
      "source": [
        "# Answer\n",
        "\n",
        "#1)\n",
        "max_p__per_load =\n",
        "print(f\"The maximum active power consumed by each load {max_p__per_load}\")\n",
        "#2)\n",
        "max_p_sgen =\n",
        "print(f\"The maximum active power generation is {max_p_sgen}\")\n",
        "\n",
        "#3)\n",
        "\n",
        "max_s_timestep =\n",
        "max_s =\n",
        "print(f\"The maximum aparent power of the load 0 is {max_s} and was found in timestep {max_s_timestep}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cf6781a",
      "metadata": {
        "id": "6cf6781a"
      },
      "source": [
        "# Running the Timeseries Calculation to Create All Labels for the Datapoints Given In the Profiles"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3aa0e73",
      "metadata": {
        "id": "e3aa0e73"
      },
      "source": [
        "The following section runs a timeseries simulation based on the active and reactive load powers, and active static generator power scenarios shown previously. The results from the powerflows solved for each timestep are stored under the Chapter6 folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66f380e5",
      "metadata": {
        "id": "66f380e5"
      },
      "outputs": [],
      "source": [
        "ds = DFData(sgen_p)\n",
        "ConstControl(net, \"sgen\", 'p_mw', element_index=net.sgen.index, profile_name=sgen_p.columns, data_source=ds)\n",
        "ds = DFData(load_p)\n",
        "ConstControl(net, \"load\", 'p_mw', element_index=net.load.index, profile_name=load_p.columns, data_source=ds)\n",
        "ds = DFData(load_q)\n",
        "ConstControl(net, \"load\", 'q_mvar', element_index=net.load.index, profile_name=load_q.columns, data_source=ds)\n",
        "\n",
        "ts.OutputWriter(net, output_path='./Chapter6/', output_file_type=\".csv\")\n",
        "ts.run_time_series.run_timeseries(net, continue_on_divergence=True )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2ae869a",
      "metadata": {
        "id": "e2ae869a"
      },
      "source": [
        "# Loading  of  simulation  results  and  generation  of  machine  learning  model datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bcb7459",
      "metadata": {
        "id": "6bcb7459"
      },
      "source": [
        "Geting the simulated loading percentage as the output to be used in the machine learning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d8ca78f",
      "metadata": {
        "scrolled": true,
        "id": "8d8ca78f"
      },
      "outputs": [],
      "source": [
        "line_loading_percentages = pd.read_csv(\"./Chapter6/res_line/loading_percent.csv\",sep=';')\n",
        "X_full = pd.concat([sgen_p, load_p, load_q], axis=1)\n",
        "\n",
        "print(X_full.head())\n",
        "X_max = X_full.max()\n",
        "print(f\"Maximum value in X_full is {X_max.max()}\")\n",
        "print(f\"Maximum index in X_full is {X_max.idxmax()}\")\n",
        "\n",
        "print(f\"Average value in X_full is {X_full.mean()}\")\n",
        "print(f\"STD value in X_full is {X_full.std()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66eaff6e",
      "metadata": {
        "id": "66eaff6e"
      },
      "source": [
        "Typically, within power grid there is no full observability of the consumption and generation at the TSO levels.\n",
        "Therefore, to incorporate this we will randomly remove some of the static geneator and load data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b030f9c3",
      "metadata": {
        "id": "b030f9c3"
      },
      "outputs": [],
      "source": [
        "observable_generator_percentage = 10.\n",
        "observable_load_percentage = 10.\n",
        "\n",
        "rng = np.random.RandomState(3065)\n",
        "range_gen_list = np.arange(0, len(sgen_p.columns))\n",
        "range_load_list = np.arange(0, len(load_p.columns))\n",
        "\n",
        "rng.shuffle(range_gen_list)\n",
        "rng.shuffle(range_load_list)\n",
        "\n",
        "known_gen_list = range_gen_list[:int(observable_generator_percentage*len(sgen_p.columns)/100)]\n",
        "known_load_list = range_load_list[:int(observable_load_percentage*len(load_p.columns)/100)]\n",
        "\n",
        "X = pd.concat([sgen_p.iloc[:, known_gen_list], load_p.iloc[:, known_load_list], load_q.iloc[:, known_load_list]], axis=1)\n",
        "print(X.shape, X_full.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2df7b3b5",
      "metadata": {
        "id": "2df7b3b5"
      },
      "source": [
        "# Task 2: Regression Task Target Set Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90f21697",
      "metadata": {
        "id": "90f21697"
      },
      "source": [
        "1) Find which is the highest line loading percentage within the dataset <br>\n",
        "2) Find which line (\"line_Y\") has the highest loading percentage within the dataset <br>\n",
        "3) Save all loading percentage values of line \"line_Y\" as the vector 'y'. The chosen line will be used as the dataset targets to be predicted by the regression problems. <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f691ff1f",
      "metadata": {
        "scrolled": true,
        "id": "f691ff1f"
      },
      "outputs": [],
      "source": [
        "#Answers\n",
        "#1),2)\n",
        "line_Y =\n",
        "max_value =\n",
        "print(f\"Highest load percentage is {max_value} and is observed on line {line_Y}\")\n",
        "\n",
        "#3)\n",
        "y =\n",
        "print(f\"The regression target set is: {y}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "532e95d3",
      "metadata": {
        "id": "532e95d3"
      },
      "source": [
        "# Task 3: Matplotlib introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4fed257",
      "metadata": {
        "id": "d4fed257"
      },
      "source": [
        "1) Plot the data labels in a line plot with x-axis as the minutes past compared to the first datapoint (remember: the dataset was created in 15 minute intervals). Add axis labels and appropriate title to the figure. The shape of the figure should have 15 inch width and 5 inch height."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8c3aa13",
      "metadata": {
        "scrolled": true,
        "id": "c8c3aa13"
      },
      "outputs": [],
      "source": [
        "#Answer\n",
        "#1)\n",
        "x_axis =\n",
        "y_axis =\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(x_axis, y_axis)\n",
        "plt.xlabel(\"timestep [m]\")\n",
        "plt.ylabel(\"loading percentage [%]\")\n",
        "plt.title(\"Label set\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a5a4d2c",
      "metadata": {
        "id": "6a5a4d2c"
      },
      "source": [
        "# Creation of a regression plotting function for later results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ef1ff5d",
      "metadata": {
        "id": "6ef1ff5d"
      },
      "outputs": [],
      "source": [
        "def plot_regression_prediciton(y_pred, y_real, title):\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.plot(y_real.index, y_pred, 'o')\n",
        "    plt.plot(y_real, 'o')\n",
        "    plt.xlabel('datapoints')\n",
        "    plt.ylabel('line loading')\n",
        "    plt.title(title)\n",
        "    plt.legend(['Prediction', 'Actual'])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d9eee4e",
      "metadata": {
        "id": "4d9eee4e"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19bbd1e1",
      "metadata": {
        "id": "19bbd1e1"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3065, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f64a695f",
      "metadata": {
        "id": "f64a695f"
      },
      "source": [
        "Standardize generally means changing the values so that the distribution’s standard deviation equals one. Scaling is often implied.\n",
        "StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance. Unit variance means dividing all the values by the standard deviation. StandardScaler results in a distribution with a standard deviation equal to 1. The variance is equal to 1 also, because variance = standard deviation squared. And 1 squared = 1.\n",
        "StandardScaler makes the mean of the distribution 0. About 68% of the values will lie be between -1 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95842d26",
      "metadata": {
        "id": "95842d26"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b8bcba7",
      "metadata": {
        "id": "4b8bcba7"
      },
      "source": [
        "# Regression: 1.Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d4cfca1",
      "metadata": {
        "id": "8d4cfca1"
      },
      "source": [
        "LinearRegression fits a linear model with coefficients w = (w1, …, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n",
        "More information in: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f88a60f",
      "metadata": {
        "id": "8f88a60f"
      },
      "outputs": [],
      "source": [
        "linear_reg = LinearRegression().fit(X_train, y_train)\n",
        "#Score is R^2\n",
        "linear_reg.score(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a92d5e32",
      "metadata": {
        "id": "a92d5e32"
      },
      "outputs": [],
      "source": [
        "y_pred_lr = linear_reg.predict(X_test)\n",
        "#Score is R^2\n",
        "linear_reg.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01c35cf2",
      "metadata": {
        "scrolled": false,
        "id": "01c35cf2"
      },
      "outputs": [],
      "source": [
        "plot_regression_prediciton(linear_reg.predict(X_train), y_train, \"Linear Regression Training Results\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1acb1c58",
      "metadata": {
        "scrolled": true,
        "id": "1acb1c58"
      },
      "outputs": [],
      "source": [
        "plot_regression_prediciton(y_pred_lr, y_test, \"Linear Regression Test Results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3516e4d6",
      "metadata": {
        "id": "3516e4d6"
      },
      "source": [
        "# Regression: 2. Support Vectors Regression (SVR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e5822f9",
      "metadata": {
        "id": "7e5822f9"
      },
      "source": [
        "More information at: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27998cca",
      "metadata": {
        "id": "27998cca"
      },
      "outputs": [],
      "source": [
        "##Hyperparameters\n",
        "\n",
        "#1. Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a\n",
        "#callable. If none is given, ‘rbf’ will be used. If a callable is given it is used to precompute the kernel matrix.\n",
        "kernel = 'rbf' #DO NOT CHANGE\n",
        "\n",
        "#2. Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.\n",
        "degree = 3 #DO NOT CHANGE\n",
        "\n",
        "#3. Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.\n",
        "gamma = 'scale' # {‘scale’, ‘auto’} or float DO NOT CHANGE\n",
        "\n",
        "#4. Independent term in kernel function. It is only significant in ‘poly’ and ‘sigmoid’.\n",
        "coef0 = 0.0 #float DO NOT CHANGE\n",
        "\n",
        "#5. Tolerance for stopping criterion.\n",
        "tol = 1e-3 # DO NOT CHANGE\n",
        "\n",
        "#6. Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive.\n",
        "#The penalty is a squared l2 penalty.\n",
        "C = 1.\n",
        "\n",
        "#7. Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training\n",
        "#loss function with points predicted within a distance epsilon from the actual value.\n",
        "epsilon = 0.1 # float DO NOT CHANGE\n",
        "\n",
        "#8. Whether to use the shrinking heuristic\n",
        "shrinking = True # bool DO NOT CHANGE\n",
        "\n",
        "#9. Hard limit on iterations within solver, or -1 for no limit.\n",
        "max_iter = 1000 # DO NOT CHANGE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67196807",
      "metadata": {
        "id": "67196807"
      },
      "source": [
        "As you can see, there are many hyperparameters which can be tuned to affect the performance of the model. In this task, you are asked only to fine tune the hyperparameter  <b>C</b> between values of </b>0.1 and 5</b>. Change the parameter value in 3 different models and document the performance differences between them.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0caabd5f",
      "metadata": {
        "id": "0caabd5f"
      },
      "outputs": [],
      "source": [
        "sv_reg = svm.SVR(kernel=kernel, C=C, epsilon=epsilon , gamma=gamma ,max_iter= max_iter).fit(X_train, y_train)\n",
        "sv_reg.score(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "652b5e65",
      "metadata": {
        "id": "652b5e65"
      },
      "outputs": [],
      "source": [
        "y_pred_svr = sv_reg.predict(X_test)\n",
        "sv_reg.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ebd33c3",
      "metadata": {
        "id": "9ebd33c3"
      },
      "outputs": [],
      "source": [
        "plot_regression_prediciton(sv_reg.predict(X_train), y_train, \"Support Vector Regression Training Results\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adc1ed5d",
      "metadata": {
        "scrolled": false,
        "id": "adc1ed5d"
      },
      "outputs": [],
      "source": [
        "plot_regression_prediciton(y_pred_svr, y_test, \"Support Vector Regression Test Results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "033d92ec",
      "metadata": {
        "id": "033d92ec"
      },
      "source": [
        "# Task 4: SVR Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "603cf697",
      "metadata": {
        "id": "603cf697"
      },
      "source": [
        "1) Change the hyperparameter 'C' between values of 0.1 and 5 in order to improve the training error. Report the resulted training and test errors for 3 different combinations. Based on your results, which model would you choose and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ca2b2af",
      "metadata": {
        "id": "4ca2b2af"
      },
      "outputs": [],
      "source": [
        "#Answers\n",
        "\n",
        "print(f\"A. For the hyperparameter C = ___ , the resulted training and test errors are {} and {}\")\n",
        "\n",
        "print(f\"B. For the hyperparameter C = ___ , the resulted training and test errors are {} and {}\")\n",
        "\n",
        "print(f\"C. For the hyperparameter C = ___ , the resulted training and test errors are {} and {}\")\n",
        "\n",
        "print(f\"My chosen hyperparameter C would be ___________________, because _____________\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7df5c20",
      "metadata": {
        "id": "d7df5c20"
      },
      "source": [
        "# Regression: 3. Decision Tree Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2265058",
      "metadata": {
        "id": "a2265058"
      },
      "outputs": [],
      "source": [
        "##Hyperparameters\n",
        "\n",
        "#.0 Random State is used to repeat same results whenever the model is run\n",
        "random_state = 3065 #DO NOT CHANGE\n",
        "\n",
        "#1. The function to measure the quality of a split. Supported criteria are “squared_error” for the mean squared error,\n",
        "#which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal\n",
        "#node, “friedman_mse”, which uses mean squared error with Friedman’s improvement score for potential splits,\n",
        "#“absolute_error” for the mean absolute error, which minimizes the L1 loss using the median of each terminal node,\n",
        "#and “poisson” which uses reduction in Poisson deviance to find splits.\n",
        "criterion = \"squared_error\" #{“squared_error”, “friedman_mse”, “absolute_error”, “poisson”}\n",
        "\n",
        "#2. The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n",
        "splitter = 'best' #{“best”, “random”}\n",
        "\n",
        "#3. The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
        "max_depth = None # int\n",
        "\n",
        "#4. The minimum number of samples required to split an internal node:\n",
        "min_samples_split = 3 # int or float\n",
        "\n",
        "#5. The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves\n",
        "#at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
        "min_samples_leaf = 1 # int or float\n",
        "\n",
        "#6. The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node.\n",
        "#Samples have equal weight when sample_weight is not provided.\n",
        "min_weight_fraction_leaf = 0. # float\n",
        "\n",
        "#7. The number of features to consider when looking for the best split:\n",
        "max_features = 10 # int or float or {\"auto, \"sqrt\", \"log2\"}, None => n_features\n",
        "\n",
        "#8. Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
        "max_leaf_nodes = None # int\n",
        "\n",
        "#9. A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
        "min_impurity_decrease = 0.0 # float\n",
        "\n",
        "#10. Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen.\n",
        "#By default, no pruning is performed.\n",
        "ccp_alpha = 0.0 # non negative float\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61e4ccf9",
      "metadata": {
        "id": "61e4ccf9"
      },
      "source": [
        "As you can see, there are many hyperparameters which can be tuned to affect the performance of the model. In this task, you are asked only to fine tune the hyperparameter  <b>min_samples_split</b> between values of </b>2 and 50</b>. Change the parameter value in 3 different models and document the performance differences between them.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54ebfcd6",
      "metadata": {
        "id": "54ebfcd6"
      },
      "outputs": [],
      "source": [
        "DT_reg = DecisionTreeRegressor(random_state=random_state, splitter=splitter, max_depth=max_depth, min_samples_split=min_samples_split,\n",
        "                               min_samples_leaf=min_samples_leaf, min_weight_fraction_leaf=min_weight_fraction_leaf, max_features=max_features,\n",
        "                               max_leaf_nodes=max_leaf_nodes, min_impurity_decrease=min_impurity_decrease, ccp_alpha=ccp_alpha)\n",
        "DT_reg.fit(X_train,y_train)\n",
        "DT_reg.score(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd736e3f",
      "metadata": {
        "id": "dd736e3f"
      },
      "outputs": [],
      "source": [
        "y_pred_dtr = DT_reg.predict(X_test)\n",
        "DT_reg.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "591d601b",
      "metadata": {
        "id": "591d601b"
      },
      "outputs": [],
      "source": [
        "plot_regression_prediciton(DT_reg.predict(X_train), y_train, \"Decision Tree Regression Training Results\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac6f40f6",
      "metadata": {
        "scrolled": true,
        "id": "ac6f40f6"
      },
      "outputs": [],
      "source": [
        "plot_regression_prediciton(y_pred_dtr, y_test, \"Decision Tree Regression Test Results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8fcd537",
      "metadata": {
        "id": "f8fcd537"
      },
      "source": [
        "# Task 5: Decision Tree Regressor Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69ad830d",
      "metadata": {
        "id": "69ad830d"
      },
      "source": [
        "1) Change the hyperparameter 'min_samples_split' in order to improve the current model (if you are not able to improve the model explain why). Report the resulted training and test errors for 3 different combinations between 2 and 50. Based on your results, which model would you choose and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "438b0cf0",
      "metadata": {
        "id": "438b0cf0"
      },
      "outputs": [],
      "source": [
        "#Answers\n",
        "\n",
        "print(f\"A. For the hyperparameter 'min_samples_split'= ____, the resulted training and test errors are {} and {}\")\n",
        "\n",
        "print(f\"B. For the hyperparameters 'min_samples_split'= ____, the resulted training and test errors are {} and {}\")\n",
        "\n",
        "print(f\"C. For the hyperparameters 'min_samples_split'= ____, the resulted training and test errors are {} and {}\")\n",
        "\n",
        "print(f\"My chosen hyperparameter 'min_samples_split' would be ___________________, because _____________\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "047f0495",
      "metadata": {
        "id": "047f0495"
      },
      "source": [
        "# Classification Label Set Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37659a91",
      "metadata": {
        "id": "37659a91"
      },
      "source": [
        "The line in the context of these lecture will be considered as within dangerous limits when it's\n",
        "loading percentage is equal or higher than 60%. Hence, the target label of each datapoint needs to be determined. Loading percentages higher than 60% will be labeled as 1, while the rest of the line loading percentages will be labeled as 0."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b176117",
      "metadata": {
        "id": "7b176117"
      },
      "source": [
        "# Task 6: Creation of Classification Label Set"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa613c58",
      "metadata": {
        "id": "aa613c58"
      },
      "source": [
        "1) Using the y_train set, create the classification labels vector y_train_class with values 0 and 1 as explained above. Print the ratio of training set safe datapoints with respect to all the training set data points <br>\n",
        "2) Using the y_test set, create the classification labels vector y_test_class with values 0 and 1 as explained above. Print the ratio of test set safe datapoints with respect to all the test set data point <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6a36b58",
      "metadata": {
        "id": "c6a36b58"
      },
      "outputs": [],
      "source": [
        "#Answers\n",
        "limit =60.\n",
        "\n",
        "\n",
        "y_train_class =\n",
        "y_test_class =\n",
        "safe_train_ratio = 100. * float(sum(y_train_class))/len(y_train_class)\n",
        "safe_test_ratio = 100. * float(sum(y_test_class))/len(y_test_class)\n",
        "\n",
        "print(f\"Out of {len(y_train_class)} Training data, the {safe_train_ratio} % are representing 'safe' line percentage\")\n",
        "print(f\"Out of {len(y_test_class)} Test data, the {safe_test_ratio} % are representing 'safe' line percentage\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05e87de6",
      "metadata": {
        "id": "05e87de6"
      },
      "source": [
        "# Classification Printing Function Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee37ee91",
      "metadata": {
        "id": "ee37ee91"
      },
      "outputs": [],
      "source": [
        "def plot_classification_prediction(y_real, y_pred_class, y_real_class, title):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(30, 5))\n",
        "    axes[0].scatter(y_real.index, y_real, c=y_pred_class, cmap=clrs.ListedColormap(['blue', 'red']))\n",
        "    axes[1].scatter(y_real.index, y_real, c=y_real_class, cmap=clrs.ListedColormap(['blue', 'red']))\n",
        "    axes[0].set_xlabel('datapoints')\n",
        "    axes[0].set_ylabel('line loading')\n",
        "    axes[1].set_xlabel('datapoints')\n",
        "    axes[1].set_ylabel('line loading')\n",
        "    axes[0].title.set_text('Predicted Classes')\n",
        "    axes[1].title.set_text('Actual Classes')\n",
        "    fig.suptitle(title, fontsize=16)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f22f9468",
      "metadata": {
        "id": "f22f9468"
      },
      "source": [
        "# Classification 1: Linear Classification - Logistic Regression Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9299949",
      "metadata": {
        "id": "c9299949"
      },
      "source": [
        "This class implements regularized logistic regression using the ‘liblinear’ library, ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ solvers. Note that regularization is applied by default. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).\n",
        "More information at: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddb1ef61",
      "metadata": {
        "id": "ddb1ef61"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "#.0 Random State is used to repeat same results whenever the model is run\n",
        "random_state = 3065 #DO NOT CHANGE\n",
        "\n",
        "#1. Specify the norm of the penalty:\n",
        "penalty = 'l2' #'l1', 'l2', ‘elasticnet’, ‘none’\n",
        "\n",
        "#2. Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver.\n",
        "#Prefer dual=False when n_samples > n_features.\n",
        "dual = False # Do not change in our case given the above inequality\n",
        "\n",
        "#3. Tolerance for stopping criterion.\n",
        "tol = 1e-3 #float\n",
        "\n",
        "#4. Inverse of regularization strength; must be a positive float.\n",
        "#Like in support vector machines, smaller values specify stronger regularization.\n",
        "C = 1.  #float\n",
        "\n",
        "#5. Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function.\n",
        "fit_intercept = True #bool\n",
        "\n",
        "#6. Useful only when the solver ‘liblinear’ is used and self.fit_intercept is set to True. In this case,\n",
        "#x becomes [x, self.intercept_scaling], i.e. a “synthetic” feature with constant value equal to intercept_scaling\n",
        "#is appended to the instance vector. The intercept becomes intercept_scaling * synthetic_feature_weight.\n",
        "intercept_scaling = 1. #float\n",
        "\n",
        "#7. Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one.\n",
        "#The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies\n",
        "#in the input data as n_samples / (n_classes * np.bincount(y)).\n",
        "class_weight = None # dict or 'balanced'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02748d99",
      "metadata": {
        "id": "02748d99"
      },
      "outputs": [],
      "source": [
        "lr_class = LogisticRegression(random_state=random_state, penalty=penalty, dual=dual, tol=tol,\n",
        "                              C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling,\n",
        "                              class_weight=class_weight)\n",
        "lr_class.fit(X_train, y_train_class)\n",
        "\n",
        "#Return the mean accuracy on the given test data and labels\n",
        "lr_class.score(X_train, y_train_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e879cadd",
      "metadata": {
        "scrolled": true,
        "id": "e879cadd"
      },
      "outputs": [],
      "source": [
        "plot_classification_prediction(y_train, lr_class.predict(X_train), y_train_class, \"Linear Regression Training Set Classification\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "524516e7",
      "metadata": {
        "id": "524516e7"
      },
      "source": [
        "The left figure plotted above illustrates the datapoints which were predicted as safe in blue, and the ones which were predicted as dangerous in red. The right figure illustrates the data point which were actually safe in blue, and the ones which were actually dangerous in red. Comparing those two graphs it is obvious that the majority of the datapoints is correctly classified but there are still some missclassification instances.\n",
        "\n",
        "To help us visualize the amount of mistclassified data points, but also to have more insights on the performance of the model with respect to each class, the confusion matrix is created below. This matrix, shows information such as how many of the actually safe (class 0 ) datapoints were identified by the model, and how many were not. Similarly, it also shows how many of the actually dangerous datapoints were identified and how many were not. Those values can be used to obtain metrics such as Precision, Recall, F1 score and Accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ae7eccb",
      "metadata": {
        "id": "4ae7eccb"
      },
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_estimator(lr_class, X_train, y_train_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd14209e",
      "metadata": {
        "id": "cd14209e"
      },
      "source": [
        "Although the previous graphs can showcase how the model performs on the training data, that performance evaluation might not be a good representation on how the model would perform to new, unseen data. Therefore, we also create the same figures for the test data, which represent a more accurate model performance to new data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22a7fa50",
      "metadata": {
        "id": "22a7fa50"
      },
      "outputs": [],
      "source": [
        "y_pred_lrc = lr_class.predict(X_test)\n",
        "lr_class.score(X_test, y_test_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82b30e3a",
      "metadata": {
        "id": "82b30e3a"
      },
      "outputs": [],
      "source": [
        "plot_classification_prediction(y_test, y_pred_lrc, y_test_class, \"Linear Regression Test Set Classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47b03741",
      "metadata": {
        "id": "47b03741"
      },
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_estimator(lr_class, X_test, y_test_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "448dd528",
      "metadata": {
        "id": "448dd528"
      },
      "source": [
        "# Task 7: Creation of SVM Classification Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96aa009e",
      "metadata": {
        "id": "96aa009e"
      },
      "source": [
        "1) Create an SVM classifier using scikit learn (Hint: Copy Implementation of Classifier 1 and then visit https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC to change model function, and hyperparameters) <br>\n",
        "2) Train the SVM classifier (Hint: Similar to Classifier 1) <br>\n",
        "3) Report training & test scores <br>\n",
        "4) Plot the classification results for the test set using the plot_classification_prediction() function <br>\n",
        "5) Plot the confusion matrix for the test set <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfa45088",
      "metadata": {
        "id": "dfa45088"
      },
      "outputs": [],
      "source": [
        "# Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Optional) Task 8: Creation of Decision Tree Classification Model\n"
      ],
      "metadata": {
        "id": "yXhYBHvwzBjf"
      },
      "id": "yXhYBHvwzBjf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Develop a Decision Tree classifier using scikit learn\n",
        "\n",
        "2) Train the Decision Tree classifier (Hint: Similar to Classifier 1)\n",
        "\n",
        "3) Report training & test scores\n",
        "\n",
        "4) Plot the classification results for the test set using the plot classification prediction() function\n",
        "\n",
        "5) Plot the confusion matrix for the test set"
      ],
      "metadata": {
        "id": "1EEtqotyzU8_"
      },
      "id": "1EEtqotyzU8_"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}